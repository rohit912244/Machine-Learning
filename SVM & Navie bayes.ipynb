{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPIoCp0OYOXIv/LqjZIxdiS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Q1. What is Information Gain, and how is it used in Decision Trees?\n","\n","Ans: Information Gain is a metric used in decision trees to measure how much a feature can reduce the entropy (or impurity) of a dataset. It is used to select the best feature to split a node, with the goal being to choose the feature that results in the most homogenous child nodes.\n","How Information Gain is used in Decision Trees   \n","-  Feature selection: At each internal node, the algorithm evaluates all candidate features to see which one provides the highest Information Gain.\n","- Splitting: The feature with the greatest Information Gain is chosen as the attribute to split the current node into subsets.\n","- Calculating Information Gain: The formula for Information Gain is the entropy of the parent node minus the weighted average entropy of the child nodes after the split.\n","- Entropy: A measure of impurity or uncertainty in a dataset. A dataset with a single class has zero entropy, while a dataset with an even mix of classes has maximum entropy.\n"],"metadata":{"id":"AHS7tFVzYcPE"}},{"cell_type":"markdown","source":["Q2. What is the difference between Gini Impurity and Entropy?\n","\n","Ans:Gini Impurity and Entropy are both metrics used in decision trees to measure the purity of a node, but they differ in their calculation and range. Gini Impurity is a faster, less computationally expensive metric that calculates the probability of misclassification, with a range of 0 to 0.5 for binary classification. Entropy measures randomness or disorder, with a range of 0 to 1, and requires more computation due to its logarithmic function."],"metadata":{"id":"a5xKdvACqFLf"}},{"cell_type":"markdown","source":["Q3. :What is Pre-Pruning in Decision Trees?\n","\n","Ans: Pre-pruning, or early stopping, is a technique in decision trees that stops the tree from growing during construction to prevent overfitting. It uses criteria like maximum depth or a minimum number of samples per split to halt the process before the tree becomes overly complex, resulting in a simpler and more interpretable model.  "],"metadata":{"id":"kePRlcGuuuzo"}},{"cell_type":"markdown","source":["Q4. Write a Python program to train a Decision Tree Classifier using Gini\n","Impurity as the criterion and print the feature importances (practical)."],"metadata":{"id":"xgbz_XqnvEHF"}},{"cell_type":"code","source":["# Import necessary libraries\n","from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data       # feature variables\n","y = iris.target     # target variable\n","\n","# Split the dataset into training and testing data (70% train, 30% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n","\n","# Create a Decision Tree Classifier with Gini Impurity as the criterion\n","clf = DecisionTreeClassifier(criterion='gini', random_state=0)\n","\n","# Train (fit) the model\n","clf.fit(X_train, y_train)\n","\n","# Print the feature importances\n","print(\"Feature Importances:\")\n","for feature_name, importance in zip(iris.feature_names, clf.feature_importances_):\n","    print(f\"{feature_name}: {importance:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WRfzFEUfwPke","executionInfo":{"status":"ok","timestamp":1761754548986,"user_tz":-330,"elapsed":71,"user":{"displayName":"Daksha Patil","userId":"18016931567776264576"}},"outputId":"84bd63ca-1d14-47f3-c28c-15dddfaa88b1"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature Importances:\n","sepal length (cm): 0.0000\n","sepal width (cm): 0.0215\n","petal length (cm): 0.3977\n","petal width (cm): 0.5808\n"]}]},{"cell_type":"markdown","source":["Q5.What is a Support Vector Machine (SVM)?\n","\n","Ans: A Support Vector Machine (SVM) is a supervised machine learning algorithm that classifies data by finding the best-fitting boundary, or hyperplane, to separate different classes. It works by identifying the hyperplane that maximizes the margin, or distance, between itself and the closest data points of each class. These closest data points are called \"support vectors,\" as they are crucial in defining the position and orientation of the hyperplane.  "],"metadata":{"id":"b8C4VMbiv7ys"}},{"cell_type":"markdown","source":["Q6.What is the Kernel Trick in SVM?\n","\n","Ans: The Kernel Trick is a mathematical technique that allows SVMs to implicitly map data into a higher-dimensional space without actually computing the transformation.\n","\n","This makes it possible to find a linear decision boundary in that high-dimensional space, which corresponds to a non-linear boundary in the original space.\n","\n","why its needed:\n","\n","- In many real-world datasets, the data is not linearly separable.\n","\n","- If you can’t separate data with a straight line (in 2D), maybe you can separate it with a curve.\n","\n","- The kernel trick allows SVMs to find such curved boundaries without explicitly computing new features."],"metadata":{"id":"pbVrHxkKwqoJ"}},{"cell_type":"markdown","source":["Q7.Write a Python program to train two SVM classifiers with Linear and RBF\n","kernels on the Wine dataset, then compare their accuracies.\n"],"metadata":{"id":"pDz76W_axrjf"}},{"cell_type":"code","source":["# Import required libraries\n","from sklearn.datasets import load_wine\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Wine dataset\n","wine = load_wine()\n","X = wine.data\n","y = wine.target\n","\n","# Split the dataset into training and testing sets (70% train, 30% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create two SVM classifiers: one with Linear kernel and one with RBF kernel\n","svm_linear = SVC(kernel='linear', random_state=42)\n","svm_rbf = SVC(kernel='rbf', random_state=42)\n","\n","# Train both models\n","svm_linear.fit(X_train, y_train)\n","svm_rbf.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred_linear = svm_linear.predict(X_test)\n","y_pred_rbf = svm_rbf.predict(X_test)\n","\n","# Compute accuracies\n","acc_linear = accuracy_score(y_test, y_pred_linear)\n","acc_rbf = accuracy_score(y_test, y_pred_rbf)\n","\n","# Print results\n","print(\"Accuracy Comparison of SVM Models:\\n\")\n","print(f\"Linear Kernel SVM Accuracy: {acc_linear:.4f}\")\n","print(f\"RBF Kernel SVM Accuracy:    {acc_rbf:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aoVXHlIVyHkM","executionInfo":{"status":"ok","timestamp":1761755047300,"user_tz":-330,"elapsed":694,"user":{"displayName":"Daksha Patil","userId":"18016931567776264576"}},"outputId":"dfb90e66-1d8f-4ac5-be53-f5f3cc06ff0c"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy Comparison of SVM Models:\n","\n","Linear Kernel SVM Accuracy: 0.9815\n","RBF Kernel SVM Accuracy:    0.7593\n"]}]},{"cell_type":"markdown","source":["Q8.What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n","\n","Ans:The Naïve Bayes classifier is a probabilistic classification algorithm that uses Bayes' Theorem with a strong \"naïve\" assumption of conditional independence among its features. It is called \"naïve\" because this assumption is often unrealistic in real-world data, but the classifier still performs well in practice, especially for tasks like spam filtering and text classification.  "],"metadata":{"id":"CuYqBJKyyL7l"}},{"cell_type":"markdown","source":["Q9.Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n","Bayes, and Bernoulli Naïve Bayes\n","\n","Ans:The main difference is the type of data each classifier handles: Gaussian Naïve Bayes is for continuous data (like height or weight), Multinomial Naïve Bayes is for discrete counts (like word frequencies in a document), and Bernoulli Naïve Bayes is for binary data (presence or absence of a feature, like if a word is in a document or not). Each model uses a different probability distribution for its features: Gaussian uses a normal distribution, Multinomial uses the multinomial distribution, and Bernoulli uses the Bernoulli distribution."],"metadata":{"id":"UN6kuIwnzEgI"}},{"cell_type":"markdown","source":["Q10. Breast Cancer Dataset\n","Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n","dataset and evaluate accuracy.\n","Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n","sklearn.datasets.\n"],"metadata":{"id":"HXFfIQu7zRl7"}},{"cell_type":"code","source":["# Import necessary libraries\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Breast Cancer dataset\n","data = load_breast_cancer()\n","X = data.data      # Features\n","y = data.target    # Target labels (0 = malignant, 1 = benign)\n","\n","# Split the dataset into training and testing sets (70% train, 30% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create a Gaussian Naïve Bayes classifier\n","gnb = GaussianNB()\n","\n","# Train the model\n","gnb.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = gnb.predict(X_test)\n","\n","# Evaluate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Display results\n","print(\"Gaussian Naïve Bayes Classifier Results\")\n","print(\"----------------------------------------\")\n","print(f\"Accuracy: {accuracy:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y7Pa5Fvoznk2","executionInfo":{"status":"ok","timestamp":1761755506972,"user_tz":-330,"elapsed":138,"user":{"displayName":"Daksha Patil","userId":"18016931567776264576"}},"outputId":"8f1a9585-9931-41a0-edab-56431f63ab54"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Gaussian Naïve Bayes Classifier Results\n","----------------------------------------\n","Accuracy: 0.9415\n"]}]}]}